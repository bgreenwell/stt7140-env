<!DOCTYPE html>
<html>
  <head>
    <title>Linear Regression</title>
    <meta charset="utf-8">
    <meta name="author" content="Brandon M. Greenwell" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <script src="libs/plotly-binding/plotly.js"></script>
    <script src="libs/typedarray/typedarray.min.js"></script>
    <script src="libs/jquery/jquery.min.js"></script>
    <link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
    <script src="libs/crosstalk/js/crosstalk.min.js"></script>
    <link href="libs/plotlyjs/plotly-htmlwidgets.css" rel="stylesheet" />
    <script src="libs/plotlyjs/plotly-latest.min.js"></script>
    <link rel="stylesheet" href="scrollable.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Linear Regression
## <br/>ES/STT 7140: Statistical Modeling for Environmental Data
### Brandon M. Greenwell

---




# A good modelling tool

.pull-left[

### At a minimum:

* Universally applicable in classification and regression
    
* Unexcelled accuracy
    
* Capable of handling large data sets
    
* Handle missing values effectively

]

--

.pull-right[

### It would also be a plus to know:

* Which variables are important?
    
* How do variables interact?
    
* What is the shape of the data (i.e., how does it cluster?)
    
* How do the features separate classes?
    
* Are their novel cases and outliers?

]
    
--

&lt;br/&gt;

.right[-Leo Breiman]


---
class: inverse, center, middle


# Statistical models


---

# Introduction

* In the one-sample `\(t\)`-test, we are interested in learning about the mean of a normal distribution/population `$$y_i \sim N\left(\mu, \sigma^2\right), \quad i = 1, 2, \dots, n$$`

* For example, `\(y\)` might represent the shell length of a randomly selected zebra mussel from a stream or lake in Michigan

* It is often convenient to think of the data `\(y_i\)` in terms of a statistical model: `$$y_i = \mu + \epsilon_i, \quad \epsilon_i \sim N\left(0, \sigma^2\right)$$`

--

    - *data* = *mean* + *remainder*
    
    - The above two expressions are mathematically equivalent
    
    - The remainder is the difference between the observed values and the mean, often refered to as the residuals

---

# Introduction

* In the two-sample `\(t\)`-test problem, we are interested in the difference between the means of two populations (or groups): `$$y_{1i} \sim N\left(\mu_1, \sigma^2\right), \quad i = 1, 2, \dots, n_1$$` `$$y_{2j} \sim N\left(\mu_2, \sigma^2\right), \quad j = 1, 2, \dots, n_2$$` `$$\delta = \mu_2 - \mu_1$$`

* As a linear model, we could use `$$y_k = \mu_1 + \delta g_k + \epsilon_k, \quad \sim N\left(0, \sigma^2\right), \quad k = 1, 2, \dots, n_1 + n_2$$`

    - Here, `\(g_k\)` is a *dummy variable* (one for `intake` group and zero for `discharge` group)

--

* Illustration: `clams.R`

* A similar approach can be used for ANOVA procedures as well


---

# Introduction

* The primary focus of this chapter is to introduce more complicated models

--

* In particular, we will focus on 

--

    - *simple linear regression* (SLR) models (i.e., linear regression with a single predictor)
    
--
    
    - *multiple linear regression* (MLR) models (i.e., linear regression with multiple predictors)
    
--
    
    - *nonlinear regression* (NLR) models

--

* In the next chapter, we will look at a more general class of regression models called *generalized linear models* (GzLMs)

    - GzLMs include both *logistic regression* and *Poisson regression* models

---
class: inverse, center, middle

# Simple linear regression


---

# The SLR model

* A regression model is a formal means of expressing the two essential ingredients of a statistical model:

    1. A tenancy of the **response variable**, `\(y\)`, with a **predictor variable**, `\(x\)`, in some systematic fashion
    
    2. A scattering of points around the hypothesized curve of statistical relationship

* These two characteristics are embodied in a regression model by postulating that:

    1. There is a **probability distribution** of `\(y\)` for each level of `\(x\)`
    
    2. The means of these probability distributions vary in some systematic fashion with `\(x\)`


---

# The SLR model

* There are many reasons for the vast popularity of regression models in statistical practice

* Regression models allow us to relate variables together in a mathematical form which can provide insight into the relationships between the variables of interest

--

* Regression models allow us to **determine statistically** if a *response variable* is related to one or more other *predictor variables* 

    - For instance, we may want to determine the effect of increasing levels of DDT on eggshell thicknesses. How does increasing levels of DDT effect eggshell thickness?

--

* Another common use of regression models is to **predict a response** 

    - For instance, if water is contaminated with a certain level of toxin, can we predict the amount of accumulation of this toxin in a fish that lives in the water? 
    

---

# The SLR model

* Many statistical applications deal with modeling how a single response variable, denoted `\(y\)`, depends on a single predictor, denoted `\(x\)`

* Illustration: [`diporeia.R`](https://github.com/bgreenwell/stt7140-env/blob/master/code/diporeia.R)

&lt;img src="figures/Diporeia-data-1.svg" width="50%" style="display: block; margin: auto;" /&gt;


---

# The SLR model

* From the previous *scatterplot*, one can see a fairly strong relationship between between the weight of the *Diporeia* and the depth of water where the Diporeia are found

* The scatterplot suggests that a straight line relationship between weight of Diporeia ($y$) and water depth ($x$) may be a reasonable way to model the data: `$$weight = \beta_0 + \beta_1 depth$$`

* Here, `\(\beta_0\)` is the `\(y\)`-intercept of the regression line and `\(\beta_1\)` is the slope (i.e., *rate of change*)

* Of course, the Diporeia data do not lie perfectly on a straight line and a probabilistic model is needed to account for the variability of points about the line:

--

`$$weight_i = \beta_0 + \beta_1 depth_i + \epsilon_i, \quad i = 1, \dots, 11$$`


---

# Assumptions of the SLR model

* The SLR model `$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i, \quad i = 1, \dots, n$$` assumes that

    1. Independent observations (i.e., the random errors are independent)
    
    2. The errors have constant variance (i.e., *homoscedasticity*)
    
    3. The errors are normally distributed (for statistical inference)
    
* If these assumptions are not met, then alternative methods need to be applied (e.g., *weighted least squares* or *mixed-effects models*)


---

# Assumptions of the SLR model

&lt;img src="figures/density-ridges-1.svg" width="60%" style="display: block; margin: auto;" /&gt;


---

# Least squares estimation

* How do we estimate the model coefficients `\(\beta_0\)` and `\(\beta_1\)`?

* There are an infinite number of lines passing through the data points `\(\left\{x_i, y_i\right\}_{i = 1}^n\)`

* The *least squares* (LS) solution seeks to find `\(\beta_0\)` and `\(\beta_1\)` that minimize the *sum of squares*: `$$SS\left(\beta_0, \beta_1\right) = \sum_{i = 1}^n\left(y_i - \beta_0 - \beta_1 x_i\right)^2$$`

* Hence, the LS line is the "best" fitting line in terms of minimizing `\(SS\left(\beta_0, \beta_1\right)\)`

--

* So, how do we minimize `\(SS\left(\beta_0, \beta_1\right)\)`?

--

    - **CALCULUS!!**


---

# Least squares estimation

* The values of `\(\beta_0\)` and `\(\beta_1\)` that minimize `\(SS\left(\beta_0, \beta_1\right)\)` are given by

    - `\(\widehat{\beta}_1 = \frac{\sum_{i = 1}^n\left(x_i - \bar{x}\right)\left(y_i - \bar{y}\right)}{\sum_{i = 1}^n\left(x_i - \bar{x}\right)^2}\)`

    - `\(\widehat{\beta}_0 = \bar{y} - \widehat{\beta}_1 \bar{x}\)`

--

* These are called the LS estimators of `\(\beta_0\)` and `\(\beta_1\)`

* Under the usual assumptions for the SLR model (normality not required), the LS estimators:

    - Are **unbiased** estimators of `\(\beta_0\)` and `\(\beta_1\)` 
    
    - Have **minimum variance** among all *linear* unbiased estimators of `\(\beta_0\)` and `\(\beta_1\)`!
    
* How do we interpret `\(\widehat{\beta}_0\)` and `\(\widehat{\beta}_1\)` for a fitted SLR model?


---

# Least squares estimation

&lt;img src="figures/Diporeia-slr-1.svg" width="70%" style="display: block; margin: auto;" /&gt;


---

# The SLR model

* The SLR model is belongs to a broad class of models called *linear models* (LMs) 

    - In an LM, the response is a **linear function of the coefficients**
    
    - Later on we'll see how to deal with nonlinear models where the response is not linearly related to the model parameters
    
* The classic two-sample `\(t\)`-test and ANOVA are linear models where the predictors are indicators for the levels of the factors involved

* In the R software, the `lm()` function can be used to fit regression models 

    - Illustration [`diporeia.R`](https://github.com/bgreenwell/stt7140-env/blob/master/code/diporeia.R)

--

* For the Diporeia example, we have `\(\widehat{y} = 0.839135 - 0.004498 x\)`, where `\(\widehat{y}\)` is the *predicted value* of `\(y\)`


---

# Inference in the SLR model

* Typically, the parameter of primary interest in SLR is the slope, `\(\beta_1\)`

* The slope measures the average rate of change in `\(y\)` relative to `\(x\)` 

* Occasionally, interest also lies in the `\(y\)`-intercept, `\(\beta_0\)`, but usually only in cases where `\(x\)` values are collected **near the origin**

* Otherwise, the `\(y\)`-intercept may not have any practical meaning

* For the Diporeia example, the estimate slope is `\(\widehat{\beta}_1 = âˆ’0.0045\)` (**how do we interpret this number?**)
    
* In SLR, it is natural to ask whether or not the **slope differs significantly from zero**. 
    
    - If the slope equals zero and the model is correctly specified, then `\(y\)` will not depend on `\(x\)` (i.e., a horizontal regression line)
    
    - If the relation is quadratic, then one could fit a straight line and get an estimated slope near zero which could be very misleading (**always plot your data**) 


---

# Inference in the SLR model

* If `\(\epsilon \sim N\left(0, \sigma^2\right)\)`, then `\(\widehat{\beta}_1 \sim N\left(\beta, \sigma_{\beta_1}^2\right)\)` (**why?**)

* The formulas for `\(\widehat{SE}\left(\widehat{\beta_0}\right)\)` and `\(\widehat{SE}\left(\widehat{\beta_1}\right)\)` are messy, but are provided my most statistical software 

    - In R, these are located in the column labeled `Std. Error` after applying the `summary()` function (e.g., `summary(slr)`)
    

---

# Inference in the SLR model
    

```r
summary(slr)
```

```
## 
## Call:
## lm(formula = weight ~ depth)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.101819 -0.056004 -0.006746  0.049235  0.151772 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.839135   0.081394  10.310 2.77e-06 ***
## depth       -0.004498   0.001382  -3.254  0.00993 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.08363 on 9 degrees of freedom
## Multiple R-squared:  0.5406,	Adjusted R-squared:  0.4896 
## F-statistic: 10.59 on 1 and 9 DF,  p-value: 0.009926
```


---

# Inference in the SLR model

* In SLR, one can test the following hypothesis: `$$H_0: \beta_1 = \beta_{10} \quad vs. \quad H_1: \beta_1 \ne \beta_{10}$$`

    - Typically, `\(\beta_{10} = 0\)`
    
* If the null hypothesis is true, then `$$t_{obs} = \frac{\widehat{\beta}_1 - \beta_{10}}{\widehat{SE}\left(\widehat{\beta_1}\right)}$$` will have a `\(t\)`-distribution with `\(n - 2\)` degrees of freedom

* From this result,we can conduct various tests of hypotheses regarding `\(\beta_1\)` and construct `\(100\left(1 - \alpha\right)\)`% confidence intervals for `\(\beta_1\)`

--

* Similar results hold for the `\(y\)`-intercept, `\(\beta_0\)`


---

# The coefficient of determination

* One of the most important statistics in regression analysis is the coefficient of determination, better known as the `\(R^2\)` ("R-squared")

* In the SLR model, `\(R^2\)` is just the square of the (Pearson) correlation between `\(x\)` and `\(y\)`

* In MLR (i.e., when we have more than one predictor), a more general definition of `\(R^2\)` is required

* The value of `\(R^2\)` is always between zero and one and **represents the proportion of variability in the response that is explained by the regression model**

--

* Illustration: [`diporeia.R`](https://github.com/bgreenwell/stt7140-env/blob/master/code/diporeia.R)


---
class: inverse, center, middle

"All models are wrong, but some are useful."

&lt;/br&gt;

-George Box


---

# Accessing the model fit

* In using the SLR to model the Diporeia data, we assumed that the relationship between `weight` and `depth` is linear

* The true *functional relationship* between `weight` and `depth` is probably not linear ðŸ˜±

* So, does that mean that the SLR model is wrong in this example? 

* Of course not, **statistical models are only approaximations to the truth!!** (it's rather odd to call an approximation wrong)

* For the Diporeia data, the SLR seems to provide a **reasonable** approximation!!

    - Other statistical models may be just as, if not more, reasonable
    
* Of course, the SLR may not provide a reasonable approximation to the truth in many cases

* The most common way to assess the fit of a regression models is through a thorough examination of the *residuals*


---

# Accessing the model fit

* Given a fitted SLR model, we can compute a prediction for the `\(i\)`-th observation: `\(\widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 x_i\)` for `\(i = 1, 2, \dots, n\)` (these are called the *fitted values*)

* Ideally, if the model fit is good, then the differences between the observed and fitted values should be small: `$$r_i = y_i - \widehat{y}_i, \quad i = 1, 2, \dots, n$$`

* These differences are referred to as the *residuals*

* One can think of the `\(i\)`-th residual as an estimated of the `\(i\)`-th error term: `\(\epsilon_i = y_i - \beta_0 - \beta_1 x_i\)`

* Recall that the errors in the SLR model are assumed to be random and have constant variance; hence, scatterplots of the residuals versus `\(x_i\)` or `\(\widehat{y}_i\)` should look like a random scatter of points!

    - In other words, if the model is specified correctly, scatterplots of the residual should not show any structure
    
    
---

# Accessing the model fit


```r
# Compute residuals
resids &lt;- weight - fitted(slr)

# Setup for side-by-side plots
par(mfrow = c(1, 2))

# Residual vs. depth
plot(depth, resids, xlab = "Depth (m)", ylab = "Residual")
abline(h = 0, lty = 2, col = "red2")  # reference line

# Residual vs. fitted value
plot(fitted(slr), resids, xlab = "Fitted value", ylab = "Residual")
abline(h = 0, lty = 2, col = "red2")  # reference line
```


---

# Accessing the model fit

&lt;img src="figures/deporeia-res-plot-1.svg" style="display: block; margin: auto;" /&gt;


---

# Accessing the model fit

* Most often, it is better to work with the *standardized residuals* and *studentized residuals*

* The *mean squared error* (MSE) provides an estimate of the constant variance, `\(\sigma^2\)`: `$$MSE = \sum_{i = 1}^n r_i^2 / (n - 2)$$`

    - The *root-mean squared error* (RMSE), `\(RMSE = \sqrt{MSE}\)`, is often used to compare fitted regression models (e.g., using `\(k\)`-fold cross-validation)

* The standardized residuals are defined as `\(r_i / \sqrt{MSE}\)`

* The studentized residuals are defined as `\(r_i / \sqrt{MSE \left(1 - h_{ii}\right)}\)`,
where `\(h_{ii}\)`, called a *hat value*, measures how far `\(x_i\)` is from the other `\(x\)` values (to be discussed more in-depth later!)


---

# Accessing the model fit


```r
# Compute MSE
mse &lt;- sum(resids ^ 2) / (length(resids) - 2)

# Compute standardized and studentized residuals
(r_stan &lt;- resids / sqrt(mse))
```

```
##           1           2           3           4           5           6 
## -1.09676754 -0.25973316  1.81482819 -0.08066253 -0.17049172  0.89367894 
##           7           8           9          10          11 
##  0.28378082 -1.07962457 -1.21751940 -0.02175600  0.93426697
```

```r
(r_stud &lt;- resids / sqrt(mse * (1 - hatvalues(slr))))
```

```
##           1           2           3           4           5           6 
## -1.23500390 -0.29246987  2.00339174 -0.08618023 -0.17978796  0.93955891 
##           7           8           9          10          11 
##  0.29781080 -1.13849222 -1.35237291 -0.02416571  1.32959703
```


---

# Accessing the model fit

&lt;img src="figures/deporeia-sres-plot-1.svg" width="65%" style="display: block; margin: auto;" /&gt;


---

# Accessing the model fit

* The studentized residuals, as defined on the previous slide, are sometimes referred to as standardized residuals ðŸ˜±:


```r
r_stud
```

```
##           1           2           3           4           5           6 
## -1.23500390 -0.29246987  2.00339174 -0.08618023 -0.17978796  0.93955891 
##           7           8           9          10          11 
##  0.29781080 -1.13849222 -1.35237291 -0.02416571  1.32959703
```

```r
rstandard(slr)  # built-in R function
```

```
##           1           2           3           4           5           6 
## -1.23500390 -0.29246987  2.00339174 -0.08618023 -0.17978796  0.93955891 
##           7           8           9          10          11 
##  0.29781080 -1.13849222 -1.35237291 -0.02416571  1.32959703
```


---

# Leave-one-out

* An *extreme point* in the predictor space tends to pull the regression lines towards itself!


---
class: center, middle

&lt;img src="figures/extreme-point-1.svg" width="100%" style="display: block; margin: auto;" /&gt;


---

# Leave-one-out

* An *extreme point* in the predictor space tends to pull the regression lines towards itself!

* This influential effect can mask points that have a strong influence on the estimated slope of the line (especially in the case where there are many predictors) since this effect will tend to make the residual less extreme

* An effective way to address this problem is to use the *jacknife*: 

    - For `\(i = 1, 2, \dots, n\)`:
    
      1) Remove the `\(i\)`-th observation

      2) Recompute the LS estimates, denoted `\(\widehat{\beta}_{0\left(i\right)}\)` and `\(\widehat{\beta}_{1\left(i\right)}\)`.
    
      3) Obtain the corresponding fitted value `\(\widehat{y}_{\left(i\right)} = \widehat{\beta}_{0\left(i\right)} + \widehat{\beta}_{1\left(i\right)} x_i\)`
      
      4) Obtain the *leave-one-out* residual `\(r_{\left(i\right)} = y_i - \widehat{y}_{\left(i\right)}\)`
      
* The `\(r_{\left(i\right)}\)`, for `\(i = 1, 2, \dots, n\)`, are known as the *PRESS* residuals
    

---

# Leave-one-out

* Fortunately, the PRESS residuals can be computed without having to refit the model `\(n\)` times: `$$r_{\left(i\right)} = r_i / \left(1 - h_{ii}\right)$$`

* The PRESS statistic `$$PRESS = \sum_{i = 1} ^ n r_{\left(i\right)} ^ 2$$` provides a useful measure of the predictive performance of the model

    - It is equivalent to *n-fold cross-validation* or *leave-one-out-cross-valudation*
    
    - `\(k\)`-fold cross validation is more general (5 and 10 are typical choices for `\(k\)`)
    
* Why bother use `\(k\)`-fold cross-validation? What's wrong with using `\(\sum_{i = 1} ^ n r_i ^ 2\)`?


---

# Walleye example

Data on Walleye fish that were caught in Butternut Lake, Wisconsin at three different periods:

.scrollable[


```r
# Load the data
url &lt;- paste0("https://raw.githubusercontent.com/bgreenwell/stt7140-env/",
              "master/data/walleye.csv")
walleye &lt;- read.csv(url)
```

```
## &lt;simpleError in parse_block(g[-1], g[1], params.src): duplicate label 'setup'&gt;
```

```r
head(walleye)
```

```
##   age   length period
## 1   1 215.2540      1
## 2   1 193.2576      1
## 3   1 202.5781      1
## 4   1 201.4597      1
## 5   1 232.0309      1
## 6   1 191.0207      1
```

]


---

# Walleye example

.scrollable[


```r
# Fit an SLR model
*walleye_slr &lt;- lm(length ~ age, data = walleye)
summary(walleye_slr)  # print model summary
```

```
## 
## Call:
## lm(formula = length ~ age, data = walleye)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -144.562  -21.966   -3.468   20.148  121.770 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 197.8332     1.0593   186.8   &lt;2e-16 ***
## age          29.6099     0.2474   119.7   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 31.82 on 3196 degrees of freedom
## Multiple R-squared:  0.8175,	Adjusted R-squared:  0.8175 
## F-statistic: 1.432e+04 on 1 and 3196 DF,  p-value: &lt; 2.2e-16
```

]


---

# Walleye example

.scrollable[

&lt;img src="figures/walleye-plots-1.svg" style="display: block; margin: auto;" /&gt;

]


---

# Estimation versus prediction

* Regression models are often used to predict a new response or estimate a mean
response for a given value of the predictor `\(x\)`

* We have seen how to compute a predicted value `\(\widehat{y} = \widehat{\beta}_0 + \widehat{\beta}_1 x\)`

* However, as with any estimate, we need a measure of reliability associated with `\(\widehat{y}_0\)`.

* Returning to the Diporeia example, the fitted regression line is given by `$$\widehat{y} = 0.83914 - 0.0045 x$$`
where `\(x\)` is the water depth and `\(y\)` is the weight of the Diporeia

* Suppose we want to predict the weight of a Diporeia at a depth of 40 meters. Then we would simply plug `\(x = 40\)` into the estimated regression equation to get a predicted value of `\(\widehat{y} = 0.83914 âˆ’ 0.0045(40) = 0.65914\)` mg.


---

# Estimation versus prediction

* Regression analysis is really a problem of estimating a conditional mean or expectation, denoted `$$E\left[y|x\right] = \beta_0 + \beta_1 x$$`

* Here, `\(E\left[y|x\right]\)` corresponds to the average value of the response `\(y\)` for all
units in the population with a specific `\(x\)` value

* Suppose we want to estimate the mean weight of the Diporeia found at a depth of 40 meters

    - For the prediction problem, we want to predict the weight of a **single Diporeia**
    
    - For the estimation problem, we want to estimate the mean of a conditional population (i.e., the population of Diporeia found at a depth of 40 meters) 
    
    - In both cases, we use `\(\widehat{y} = 0.65914\)` as the predicted weight and as the estimate of the mean weight of Diporeia found at a depth of 40 meters
    
* In other words, the point estimate is th same for prediction and estimation, the difference lies in the estimated standard error of each!


---

# Estimation versus prediction

* There is more uncertainty associated with prediction a single new observation (**why?**)

* For a given value of `\(x\)`, it is customary to compute **confidence intervals for an estimated mean response** and a **prediction interval for a single new response value**

* The idea of a prediction interval is to determine an interval that will contain a certain percentage of the population

    - Because a prediction interval is attempting to capture a single, random future response, as opposed to the mean of the conditional population, it will be wider than the associated confidence interval
    
* Formulas on board (and in text)


---

# Examples

* [Using the `predict()` function in R](https://github.com/bgreenwell/stt7140-env/blob/master/code/diporeia.R)

* [Anscombe's quartet of "identical" simple linear regressions](https://github.com/bgreenwell/stt7140-env/blob/master/code/anscombe.R)


---

# Cautionary notes

* Extrapolation

* Outliers and robust regression


---
class: inverse, center, middle

# Multiple linear regression


---

# Background

* The multiple linear regression (MLR) model generalizes the SLR model to a model with `\(p &gt; 1\)` predictors: `$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots \beta_p x_p + \epsilon$$`

* Typical assumptions include:

  1. Independent observations
  
  2. Constant error variance
  
  3. The error term has a normal distribution
  
* The coefficients can still be estimated using the least-squares method

* The interpretation of the regression coefficients is similar to that of the slope in SLR, but trickier


---

# Coefficient of determination

* Similar to ANOVA, the total variability in `\(y\)` (SST) can be partitioned into two components:

    - The variability due to the regression (SSR)
    
    - The variability due to error (SSE)
    
* SST = SSR + SSE

* The general definition for the coefficient of determination is `$$R ^ 2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$$`

    - Generally, we want `\(R ^ 2\)` to be "high", but the definition of "high" depends on the application
    
    - `\(R ^ 2\)` can be arbitrarily inflated by adding more predictors (regardless of their association with `\(y\)`)
    
    - `\(R_{adj} ^ 2 = 1 - \left(\frac{n - 1}{n - p}\right) \frac{SSE}{SST}\)`
    
    
---

# The regression `\(F\)`-test

* Just as in ANOVA, an `\(F\)`-test can be conducted by comparing SSR and SSE

    - `\(H_0: \beta_1 = \beta_2 = \dots = \beta_p = 0 \quad vs. \quad H_1: \text{not so}\)`
    
    - `\(F_{obs} = \frac{SSR / p}{SSE / \left(n - p - 1\right)}\)`
    
    - If `\(H_0\)` is true, then `\(F_{obs}\)` will have come from an `\(F\)`-distribution with `\(p\)` numerator degrees of freedom and `\(n - p - 1\)` denominator degrees of freedom 
    
* The individual coefficients can also be tested using the usual `\(t\)`-test approach (on the board)

* Beware of the *multiple testing problem*

    - Whenever tests (or confidence intervals) for multiple parameters are considered, some sort of multiplicity correction should be made (i.e., *Bonferroni* or *Scheffes method*)
    
    - This is common when constructing confidence/prediction bands for a fitted regression curve
    
    
---

## Partial `\(F\)`-tests

* We can also use `\(F\)`-tests to test if a specific subset of the regression coefficients are all zero

    - This can be accomplished by using the sums of squares from both the full and reduced models (details omitted, but can be found in the online notes)

    - In R, this is trivial using the `anova()` function:

```r
full_model &lt;- lm(y ~ x1 + x2 + x3 + x4)
reduced_model &lt;- lm(y ~ x1 + x4)
anova(reduced_model, full_model)
```

* This can be useful for model building, but better techniques are available (e.g., *forward*, *backward*, and *stagewise regression*)

* [mercury.R](https://github.com/bgreenwell/stt7140-env/blob/master/code/mercury.R)

---

## Polynomial regression

* Often times, the relationship between `\(x\)` and `\(y\)` is *nonlinear*: `\(y = f\left(x\right) + \epsilon\)`

* In these cases, SLR may not provide an adequate fit

* *Polynomial models* offer a flexible way to modeling nonlinear relationships (due to *Taylor's theorem*), but as we will see, should be **used with caution** `$$E\left(y|x\right) = \beta_0 + \beta_1x + \beta_2 x^2 + \dots + \beta_p x^p$$`

* Note that polynomial regression is just a special case of MLR

* In R, we have to use the `I()` function to add polynomial terms, for example

```r
fit &lt;- lm(y ~ x + I(x ^ 2))
```
will fit a quadratic model of the form `$$E\left(y|x\right) = \beta_0 + \beta_1 x + \beta_2 x^2$$`

---

## Polynomial regression

.pull-left[

Ashton et al. (2007) measured the carapace length (in mm) of 18 female gopher tortoises (Gopherus polyphemus) in Okeeheelee County Park, Florida, and X-rayed them to count the number of eggs in each.

* Ashton, K.G., R.L. Burke, and J.N. Layne. 2007. Geographic variation in body and clutch size of gopher tortoises. Copeia 2007: 355-363.

* [turtles.R](https://github.com/bgreenwell/stt7140-env/blob/master/code/turtles.R)

]

.pull-right[

&lt;img src="figures/xray.png" width="367" style="display: block; margin: auto;" /&gt;

]


---

## Multicolinearity

* In many (typically nonexperimental) situations, the predictor variables tend to be correlated among themselves

* When this correlation is "high", *multicollinearity* is said to exist

* Multicollinearity does not, in general, prevent us from obtaining a good fit! It can, however, cause other issues:

    - Multicollinearity can cause some of the estimated coefficients to become unstable (i.e., high standard errors)
    
    - Multicollinearity can complicate the interpretation of the estimated coefficients (e.g., predicting crop yield from the amount of rainfall and hours of sunshine)
    
* A simple way to assess whether or not multicollinearity is present is to use *variance inflation factors* (VIFs)

    - VIFs are vailable from the R package [`car`](https://cran.r-project.org/package=car) as the next example illustrates


---

## Heart catheter example

A study was conducted and data collected to fit a regression model to predict the length of a catheter needed to pass from a major artery at the femoral region and moved into the heart for children (Weisberg 1980). For 12 children, the proper catheter length was determined by checking with a fluoroscope that the catheter tip had reached the right position. The goal is to determine a model where a childâ€™s height and weight could be used to predict the proper catheter length.

* [heart.R](https://github.com/bgreenwell/stt7140-env/blob/master/code/heart.R)


---

## Heart catheter example


```r
# Load required packages
library(car)     # for vif() function
library(plotly)  # for interactive plotting

# Load the data
url &lt;- paste0("https://raw.githubusercontent.com/bgreenwell/",
              "stt7140-env/master/data/heart")
heart &lt;- read.table(url, header = TRUE)
```


---

## Heart catheter example

.scrollable[


```r
# Scatterplot matrix
pairs(heart)
```

&lt;img src="figures/unnamed-chunk-4-1.svg" style="display: block; margin: auto;" /&gt;

```r
# Correlation matrix
(cor_mat &lt;- cor(heart))
```

```
##           Height    Weight    Length
## Height 1.0000000 0.9610936 0.8927873
## Weight 0.9610936 1.0000000 0.9045102
## Length 0.8927873 0.9045102 1.0000000
```

]

---

## Heart catheter example

.scrollable[


```r
# Interactive 3-D scatterplot
p &lt;- plot_ly(heart, x = ~Height, y = ~Weight, z = ~Length) %&gt;%
  add_markers() %&gt;%
  layout(scene = list(xaxis = list(title = "Weight (pounds)"),
                      yaxis = list(title = "Height (inches)"),
                      zaxis = list(title = "Length (cm)")))
p
```

<div id="d456309ea7d9" style="width:504px;height:504px;" class="plotly html-widget"></div>
<script type="application/json" data-for="d456309ea7d9">{"x":{"visdat":{"d456430ba19a":["function () ","plotlyVisDat"]},"cur_data":"d456430ba19a","attrs":{"d456430ba19a":{"x":{},"y":{},"z":{},"alpha":1,"sizes":[10,100],"type":"scatter3d","mode":"markers"}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"Weight (pounds)"},"yaxis":{"title":"Height (inches)"},"zaxis":{"title":"Length (cm)"}},"xaxis":{"domain":[0,1]},"yaxis":{"domain":[0,1]},"hovermode":"closest","showlegend":false},"source":"A","config":{"modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"data":[{"x":[42.8,63.5,37.5,39.5,45.5,38.5,43,22.5,37,23.5,33,58],"y":[40,93.5,35.5,30,52,17,38.5,8.5,33,9.5,21,79],"z":[37,50,34,36,43,28,37,20,34,30,38,47],"type":"scatter3d","mode":"markers","marker":{"fillcolor":"rgba(31,119,180,1)","color":"rgba(31,119,180,1)","line":{"color":"transparent"}},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>

]


---

## Heart catheter example

.scrollable[


```r
# Fit a linear model
fit &lt;- lm(Length ~ Height + Weight, data = heart)
summary(fit)
```

```
## 
## Call:
## lm(formula = Length ~ Height + Weight, data = heart)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.7419 -1.2034 -0.2595  1.8892  6.6566 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)  20.3758     8.3859   2.430    0.038 *
## Height        0.2107     0.3455   0.610    0.557  
## Weight        0.1911     0.1583   1.207    0.258  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.778 on 9 degrees of freedom
## Multiple R-squared:  0.8254,	Adjusted R-squared:  0.7865 
## F-statistic: 21.27 on 2 and 9 DF,  p-value: 0.0003888
```

```r
# Compute variance inflation factors
*vif(fit)
```

```
##   Height   Weight 
## 13.10633 13.10633
```

]


---

## Heart catheter example

* Both variance inflation factors are greater than 10 ðŸ˜±

* What are some potential remedies?


---

## Interaction effects

* A regression model is called *additive* is the mean response has the form `$$E\left(y\right) = \sum_{i = 1}^p f_i\left(x_i\right),$$` where the `\(f_1, f_2, \dots, f_p\)` can be any functions

    - In this case, we say that the effects of each `\(x_i\)` on `\(y\)` are additive

--

* **Question:** is the expression `\(\beta_0 + \beta_1 x_1 + \beta_2 x_1^2 + \beta_3 x_2^2\)` additive?
    
--

* In contrast, the following expression is not additive `$$E\left(y\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2$$`

   - We refer to the product `\(x_1 x_2\)` as an *interaction effect*
   
* The inclusion of interaction effects changes the interpretation of the regression coefficidents!

* Interaction effects represent *curvature* in the estimated mean response


---
class: inverse, center, middle

## What are some of the potential issues with including interaction effects?


---

## Categorical predictors

* Oftentimes some of the predictors of interest are categorical in nature (e.g., sex = {male, female})

* In statistcis, we call categorical variables *factors* and there associated values *levels* (e.g., `sex` is a factor with possible levels `"male"` and `"female"`)

* In regression models, categorical variables have to be coded numerically (e.g., using numeric indicators for the different levels)

* Although there are a number of ways in which to encode factors, we discuss one: *dummy variable* encoding


---

## Dummy encoding for factors

* For a factor with `\(k\)` levels, we use `\(k - 1\)` indicator variables

    - One (binary) indicator variable for `\(k - 1\)` of the `\(k\)` levels
    
    - **Question:** Why not create an indicator for all `\(k\)` of the levels?

* For example, suppose `\(x\)` represents geographic location with three possible values: `\(A\)`, `\(B\)`, and `\(C\)`

    - Here, `\(x\)` is a factor with `\(k = 3\)` levels

* In this case, we would create `\(k - 1 = 2\)` new predictors encoded as follows:

    - `$$x_B = \begin{cases} 1 &amp; \quad \text{if } x = B \\ 0 &amp; \quad \text{otherwise} \end{cases}$$`
    
    - `$$x_C = \begin{cases} 1 &amp; \quad \text{if } x = C \\ 0 &amp; \quad \text{otherwise} \end{cases}$$`

* In R this is done automatically


---

## Interpreting categorical predictors

* Here, factor level `\(A\)` is referred to as the *reference level*

* If `\(x\)` is the only predictor, then the regression model is `$$E\left(y\right) = \beta_0 + \beta_1 x_B + \beta_2 x_C$$`

* Essentially, we have three models (one for each location):

    - For location `\(B\)`, we have `\(E\left(y\right) = \beta_0 + \beta_1 \times 1 + \beta_2 \times 0 = \beta_0 + \beta_1\)`
    
    - For location `\(C\)`, we have `\(E\left(y\right) = \beta_0 + \beta_1 \times 0 + \beta_2 \times 1 = \beta_0 + \beta_2\)`
    
    - For location `\(A\)`, we have `\(E\left(y\right) = \beta_0 + \beta_1 \times 0 + \beta_2 \times 0 = \beta_0\)`
    
* **Question:** What is another term for the model in this particular example?


---

## Interpreting categorical predictors


```r
clams &lt;- read.table(paste0("https://raw.githubusercontent.com/bgreenwell/",
                           "stt7140-env/master/data/clams.csv"),
                    header = TRUE)
head(clams)
```

```
##   obs site length width height
## 1   1    1   7.20  6.10   4.45
## 2   2    1   7.50  5.90   4.65
## 3   3    1   6.89  5.45   4.00
## 4   4    1   6.95  5.76   4.02
## 5   5    1   6.73  5.36   3.90
## 6   6    1   7.25  5.84   4.40
```


---

## Interpreting categorical predictors

.scrollable[

* Proposed model: `\(E\left(height\right) = \beta_0 + \beta_1 site\)`

* Using dummy-variable enoding, we have: `\(E\left(height\right) = \beta_0 + \beta_1 I\left(site = 2\right) + \beta_2 I\left(site = 3\right)\)`


```r
# Linear model
clams_lm &lt;- lm(height ~ as.factor(site), data = clams)
summary(clams_lm)  # print summary output
```

```
## 
## Call:
## lm(formula = height ~ as.factor(site), data = clams)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.4188 -0.2677  0.0698  0.3012  0.7504 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       4.29520    0.08434  50.926   &lt;2e-16 ***
## as.factor(site)2 -0.19562    0.12051  -1.623    0.109    
## as.factor(site)3 -0.17640    0.11928  -1.479    0.144    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.4217 on 71 degrees of freedom
## Multiple R-squared:  0.04363,	Adjusted R-squared:  0.01669 
## F-statistic:  1.62 on 2 and 71 DF,  p-value: 0.2052
```

```r
# Residual plots
par(mfrow = c(1, 2))
plot(clams_lm, which = 1:2)
```

&lt;img src="figures/unnamed-chunk-8-1.svg" width="100%" style="display: block; margin: auto;" /&gt;

```r
# ANOVA table
anova(clams_lm)
```

```
## Analysis of Variance Table
## 
## Response: height
##                 Df  Sum Sq Mean Sq F value Pr(&gt;F)
## as.factor(site)  2  0.5761 0.28803  1.6196 0.2052
## Residuals       71 12.6268 0.17784
```

]


---

## Analysis of covariance

* In many statistical studies, the goal is to compare two or more groups in terms of a continuous response `\(y\)` (e.g., the two-sample `\(t\)`-test or ANOVA)

* Oftentimes, however, additional information in the form of a continuous variable `\(x\)` may available to help in the comparison

    - Ideally, `\(x\)` will be correlated with `\(y\)`
    
* Our main interest lies in comparing the populations, but we would like to take into account the additional information contained in `\(x\)`

    - In this case, we call `\(x\)` a covariate

* We'll illustrate with an example


---

## Fruitfly example

**It has been established that increased reproduction reduces longevity in female fruit flies. A study was conducted to see if the same effect exists for male fruit flies** (Hanley and Shapiro, 1994). The experiment consisted of five groups: males forced to (i) live alone, (ii) to live with one pregnant female, (iii) to live with eight pregnant females, (iv) to live with one fertile female, and (v) to live with eight fertile females. **The response of interest is `lifespan` (measured in days).** **Variables also measured were `thorax` length (mm), and** the percentage of each day spent sleeping. For our analysis, we will only focus on two groups: control group of males living with one pregnant female and an experiment group of males living with one fertile female; these are stored in **the factor variable `group` with levels `"control"` and `"treatment"`.** 


---

## Loading the data

.scrollable[


```r
# Load the data
url &lt;- paste0("https://raw.githubusercontent.com/bgreenwell/",
              "stt7140-env/master/data/fruitfly.csv")
fruitfly &lt;- read.csv(url)
fruitfly
```

```
##    lifespan     group thorax
## 1        46   control   0.64
## 2        42   control   0.68
## 3        65   control   0.72
## 4        46   control   0.76
## 5        58   control   0.76
## 6        42   control   0.80
## 7        48   control   0.80
## 8        58   control   0.80
## 9        50   control   0.82
## 10       80   control   0.82
## 11       63   control   0.84
## 12       65   control   0.84
## 13       70   control   0.84
## 14       70   control   0.84
## 15       72   control   0.84
## 16       97   control   0.84
## 17       46   control   0.88
## 18       56   control   0.88
## 19       70   control   0.88
## 20       70   control   0.88
## 21       72   control   0.88
## 22       76   control   0.88
## 23       90   control   0.88
## 24       76   control   0.92
## 25       92   control   0.92
## 26       21 treatment   0.68
## 27       40 treatment   0.68
## 28       44 treatment   0.72
## 29       54 treatment   0.76
## 30       36 treatment   0.78
## 31       40 treatment   0.80
## 32       56 treatment   0.80
## 33       60 treatment   0.80
## 34       48 treatment   0.84
## 35       53 treatment   0.84
## 36       60 treatment   0.84
## 37       60 treatment   0.84
## 38       65 treatment   0.84
## 39       68 treatment   0.84
## 40       60 treatment   0.88
## 41       81 treatment   0.88
## 42       81 treatment   0.88
## 43       48 treatment   0.90
## 44       48 treatment   0.90
## 45       56 treatment   0.90
## 46       68 treatment   0.90
## 47       75 treatment   0.90
## 48       81 treatment   0.90
## 49       48 treatment   0.92
## 50       68 treatment   0.92
```

]


---

## Visualizing the data

&lt;img src="figures/unnamed-chunk-10-1.svg" width="60%" style="display: block; margin: auto;" /&gt;


---

# A two-sample `\(t\)`-test

.scrollable[


```r
# Two-sample t-test
t.test(lifespan ~ group, data = fruitfly)
```

```
## 
## 	Welch Two Sample t-test
## 
## data:  lifespan by group
## t = 1.8585, df = 47.893, p-value = 0.06925
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.6585072 16.7385072
## sample estimates:
##   mean in group control mean in group treatment 
##                   64.80                   56.76
```

```r
# Linear model (equivalent)
summary(lm(lifespan ~ group, data = fruitfly))
```

```
## 
## Call:
## lm(formula = lifespan ~ group, data = fruitfly)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -35.76  -8.79   0.20  10.46  32.20 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      64.800      3.059  21.184   &lt;2e-16 ***
## grouptreatment   -8.040      4.326  -1.859   0.0692 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 15.29 on 48 degrees of freedom
## Multiple R-squared:  0.06713,	Adjusted R-squared:  0.0477 
## F-statistic: 3.454 on 1 and 48 DF,  p-value: 0.06923
```

]


---

## Full model

.scrollable[


```r
# Full model
fit1 &lt;- lm(lifespan ~ thorax + group + thorax * group, 
           data = fruitfly)

# Print model summary
summary(fit1)
```

```
## 
## Call:
## lm(formula = lifespan ~ thorax + group + thorax * group, data = fruitfly)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -25.9509  -9.2539   0.9361   7.3027  30.3071 
## 
## Coefficients:
##                       Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)            -43.725     29.766  -1.469 0.148655    
## thorax                 131.450     35.931   3.658 0.000651 ***
## grouptreatment         -14.267     42.200  -0.338 0.736836    
## thorax:grouptreatment    5.551     50.575   0.110 0.913072    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 12.3 on 46 degrees of freedom
## Multiple R-squared:  0.4217,	Adjusted R-squared:  0.384 
## F-statistic: 11.18 on 3 and 46 DF,  p-value: 1.244e-05
```

]


---

## Reduced model

.scrollable[


```r
# Thorax only
fit2 &lt;- lm(lifespan ~ thorax, data = fruitfly)

# Print model summary
summary(fit2)
```

```
## 
## Call:
## lm(formula = lifespan ~ thorax, data = fruitfly)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -24.111  -8.825   1.207   7.707  35.143 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   -45.82      22.22  -2.062   0.0447 *  
## thorax        128.18      26.63   4.813 1.52e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 13.01 on 48 degrees of freedom
## Multiple R-squared:  0.3255,	Adjusted R-squared:  0.3115 
## F-statistic: 23.17 on 1 and 48 DF,  p-value: 1.519e-05
```

```r
# Compare models
anova(fit2, fit1)
```

```
## Analysis of Variance Table
## 
## Model 1: lifespan ~ thorax
## Model 2: lifespan ~ thorax + group + thorax * group
##   Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  
## 1     48 8118.4                              
## 2     46 6961.1  2    1157.3 3.8239 0.02909 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

]


---

## Testing for parallel slopes

.scrollable[


```r
# Parallel regression lines
fit3 &lt;- lm(lifespan ~ thorax + group, data = fruitfly)

# Print model summary
summary(fit3)
```

```
## 
## Call:
## lm(formula = lifespan ~ thorax + group, data = fruitfly)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -26.103  -9.123   1.092   7.273  30.267 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     -46.038     20.799  -2.214  0.03175 *  
## thorax          134.252     25.019   5.366 2.42e-06 ***
## grouptreatment   -9.651      3.456  -2.793  0.00753 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 12.17 on 47 degrees of freedom
## Multiple R-squared:  0.4215,	Adjusted R-squared:  0.3969 
## F-statistic: 17.12 on 2 and 47 DF,  p-value: 2.593e-06
```

```r
# Compare models
anova(fit3, fit1)
```

```
## Analysis of Variance Table
## 
## Model 1: lifespan ~ thorax + group
## Model 2: lifespan ~ thorax + group + thorax * group
##   Res.Df    RSS Df Sum of Sq     F Pr(&gt;F)
## 1     47 6962.9                          
## 2     46 6961.1  1    1.8233 0.012 0.9131
```

]


---

## The final model

&lt;img src="figures/unnamed-chunk-15-1.svg" width="60%" style="display: block; margin: auto;" /&gt;
    </textarea>
<script src="libs/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();
(function(time) {
  var d2 = function(number) {
    return ('0' + number).slice(-2); // left-pad 0 to minutes/seconds
  },

  time_format = function(total) {
    var secs = Math.abs(total) / 1000;
    var h = Math.floor(secs / 3600);
    var m = Math.floor(secs % 3600 / 60);
    var s = Math.round(secs % 60);
    var res = d2(m) + ':' + d2(s);
    if (h > 0) res = h + ':' + res;
    return res;  // [hh:]mm:ss
  },

  slide_number_div = function(i) {
    return document.getElementsByClassName('remark-slide-number').item(i);
  },

  current_page_number = function(i) {
    return slide_number_div(i).firstChild.textContent;  // text "i / N"
  };

  var timer = document.createElement('span'); timer.id = 'slide-time-left';
  var time_left = time, k = slideshow.getCurrentSlideIndex(),
      last_page_number = current_page_number(k);

  setInterval(function() {
    time_left = time_left - 1000;
    timer.innerHTML = ' ' + time_format(time_left);
    if (time_left < 0) timer.style.color = 'red';
  }, 1000);

  slide_number_div(k).appendChild(timer);

  slideshow.on('showSlide', function(slide) {
    var i = slide.getSlideIndex(), n = current_page_number(i);
    // reset timer when a new slide is shown and the page number is changed
    if (last_page_number !== n) {
      time_left = time; last_page_number = n;
      timer.innerHTML = ' ' + time_format(time); timer.style.color = null;
    }
    slide_number_div(i).appendChild(timer);
  });
})(60000);</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
