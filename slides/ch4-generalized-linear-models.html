<!DOCTYPE html>
<html>
  <head>
    <title>Generalized Linear Models (GzLMs)</title>
    <meta charset="utf-8">
    <meta name="author" content="Brandon M. Greenwell" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="scrollable.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Generalized Linear Models (GzLMs)
## <br/>ES/STT 7140: Statistical Modeling for Environmental Data
### Brandon M. Greenwell

---




## Introduction

* In ordinary linear regression, we assume that the response `\(y\)` is continuous (and normally distributed)

* In many real word regression problems, however, the response may not be normally distributed, or even continuous!

* For example, many response variables of interest are binary (e.g., `"survived"` or `"died"`, `"infected"` or `"not infected"`)

* Another common type of response is a count (e.g., the number of eggs in a nest or number of vines on a tree)

    - Although not normal, we can use the *Poisson* and *negative binomial* distributions to model counts
    
* In this chapter, we introduce GzLMs for accommodating many types of non-normal response variables

* We start with the most common type of GzLM for handling binary outcomes


---
class: inverse, center, middle


# Logistic Regression

---

## Binary outcomes

* Consider an experiment where the measured outcome of interest is either a success or failure (which can be coded as 1 and 0, respectively)

* The probability of a success or failure may depend on a set of predictor variables

* One idea on how to model such data is to simply fit a regression with the goal of
estimating the probability of success given some values of the predictor 

* However, this approach will not work because probabilities are constrained to fall between 0 and 1

* In the classical regression setup with a continuous response, the predicted values can range over all real numbers

* For binary outcomes, a different modeling technique is needed


---

## Binary outcomes

* Recall that fitting an SLR model is equivalent to estimating a conditional mean of the form `$$E\left(y|x\right) = \beta_0 + \beta_1 x$$`

* In logistic regression, we have `\(E\left(y|x\right) = P\left(y = 1|x\right)\)`

    - In other words, in logistic regression, we are estimating a conditional probability
    
* The goal in logistic regression is to model the probability that `\(y = 1\)`

    - If we know `\(P\left(y = 1|x\right)\)` then we also know `\(P\left(y = 0|x\right) = 1 - P\left(y = 1|x\right)\)`


---

## Dobson's beetle data

Beetles were exposed to gaseous carbon disulphide at various concentrations
(in mf/L) for five hours (Bliss, 1935) and the number of beetles killed were
noted. The data are available in the [`investr`](https://cran.r-project.org/package=investr) package.


```r
*# install.packages("investr")
data(beetle, package = "investr")
beetle
```

```
##    ldose  n  y
## 1 1.6907 59  6
## 2 1.7242 60 13
## 3 1.7552 62 18
## 4 1.7842 56 28
## 5 1.8113 63 52
## 6 1.8369 59 53
## 7 1.8610 62 61
## 8 1.8839 60 60
```


---

## Dobson's beetle data


```r
plot(y/n ~ ldose, data = beetle, pch = 19, cex = 1.2)
```

&lt;img src="figures/unnamed-chunk-2-1.svg" style="display: block; margin: auto;" /&gt;


---

## Binomial experiments

* **Binomial experiments** are experiments which consist of:

    (1) `\(n\)` *independent* trials
    
    (2) Each trial results in a binary outcome (typically defined as success or failure)
    
    (3) The probability of sucess, denoted `\(p\)`, is the same for each trial
    
* Can you think of a simple binomial experiment?

* The **binomial distribution** describes the probability of observing `\(k\)` successes in a binomial experiment with `\(n\)` (independent) trials and probability of success `\(p\)`

    - The binomial distribution is denoted `\(Bin\left(n, p\right)\)`


---

## Logistic regression

* Logistic regression goes a step further by allowing the probability of success to be a function of `\(x\)` (i.e., a binomial distribution where `\(p\)` depends on `\(x\)`)

* In (simple) logistic regression, we model the probability of success as a function of `\(x\)` using `$$p\left(x\right) = \frac{\exp\left(\beta_0 + \beta_1 x\right)}{1 + \exp\left(\beta_0 + \beta_1 x\right)}$$`

    - If `\(\beta_1 = 0\)` then `\(p\left(x\right)\)` is a constant
    
    - If `\(\beta_1 &lt; 0\)`, then `\(p\left(x\right)\)` is an increasing function in `\(x\)` (and vice versa if `\(\beta_1 &lt; 0\)`)
    
    - For a particular probability of success `\(p\)`, the *odds of success* is defined as `\(p / \left(1 - p\right)\)`

--

* In logistic regression, we are modelling the `\(\log\left(odds\right)\)`, also referred to as the *logit*: `$$\log\left[\frac{p\left(x\right)}{1 - p\left(x\right)}\right] = \beta_0 + \beta_1 x = \log\left[odds\left(x\right)\right]$$`


---

## Maximum likelihood estimation

* The question now is "How do we estimate `\(\beta_0\)` and `\(\beta_1\)`?"

* The method of *maximum likelihood* (ML) estimation is a general technique for estimating the parameters of a distribution from observed data

* For linear regression with normally distributed errors the ML estimates of the parameters are equivalent to the LS estimates

    - ML estimation also provides an estimate of `\(\sigma\)` in linear regression!

* A general overview of ML estimation is beyond the scope of this class, but we'll illustrate with a simple example: 

    - Finding the ML estimator of `\(p\)` in a binomial experiment

* Typically, however, closed-form solutions are not available for most problems and iterative numerical techniques are required in finding ML estimates (e.g., the *Newton-Raphson algorithm* and *iterative re-weighted least squares*)

---

## Logistic regression in R

.scrollable[

* In R, we can fit the logistic regression models using the `glm()` function

* `glm()` is a general function for fitting all kinds of GzLMs (including ordinary linear regression models!)

* To specify a GzLM you need to specify some additional arguments (the most important being the `family` argument)

* For logistic regression we always specify `family = binomial` in the call to `glm()`

* The template for simple logistic regression is

```r
fit &lt;- glm(y ~ x, data = &lt;data-name&gt;, family = binomial)
```
where `y` is a factor with the first level denoting a "success" or 

```r
fit &lt;- glm(cbind(y, n-y) ~ x, data = &lt;data-name&gt;, family = binomial)
```
where `n` and `y` are the number of trials and successes, respectively, in a binomial experiment

]


---

## Dobson's beetle data

.scrollable[


```r
fit &lt;- glm(
* cbind(y, n-y) ~ ldose,
  data = beetle,
* family = binomial(link = "logit")
)
summary(fit)
```

```
## 
## Call:
## glm(formula = cbind(y, n - y) ~ ldose, family = binomial(link = "logit"), 
##     data = beetle)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.5941  -0.3944   0.8329   1.2592   1.5940  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -60.717      5.181  -11.72   &lt;2e-16 ***
## ldose         34.270      2.912   11.77   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 284.202  on 7  degrees of freedom
## Residual deviance:  11.232  on 6  degrees of freedom
## AIC: 41.43
## 
## Number of Fisher Scoring iterations: 4
```

]


---

## Predictions from a logistic regression

* In logistic regression, we can make predictions on one of two scales:

    - The probability scale (i.e., generating predicted probabilities)
    
    - The logit scale
    
* In R, we can obtain such predictions using the `predict()` function with the `type` option, for example

    - `predict(fit, type = "response")` (for predicted probabilities)

    - `predict(fit, type = "link")` (for predicted logits)


---

## Dobson's beetle data


```r
investr::plotFit(fit, pch = 19, col = "red2", cex = 1.2, lwd = 2)
```

&lt;img src="figures/unnamed-chunk-6-1.svg" width="80%" style="display: block; margin: auto;" /&gt;


---

## Interpeting the coefficients

* So how do we interpret `\(\beta_0\)` and `\(\beta_1\)` in the (simple) logistic regression model?

* Recall that in the SLR model, `\(\beta_1\)` represents the rate of change in the mean response per one-unit increase in `\(x\)`

* In the (simple) logistic regression model, however, we don't interpret `\(\beta_1\)` directly, but rather, `\(\exp\left(\beta_1\right)\)`

* As it turns out, for a continuous predictor `\(x\)`, `$$OR = \frac{odds\left(x + 1\right)}{odds\left(x\right)} = \exp\left(\beta_1\right)$$`

* Hence, for a continuous predictor `\(x\)`, `\(\exp\left(\beta_1\right)\)` represents the **multiplicative** increase in the odds of success per one-unit increase in `\(x\)`


---

## Inference for the slope

* As was the case with SLR, we are often concerned with the statistical significance of the slope; in particular, whether or not it statistically significantly differs from zero: `$$H_0: \beta_1 = 0 \quad versus \quad H_1: \beta_1 \ne 0$$`

* The simplest approach is to use a *Wald test*

* The Wald test for `\(\beta_1\)` in the logistic regression model is based on the test statistic `$$z_{obs} = \frac{\widehat{\beta}_1}{\widehat{SE}\left(\widehat{\beta}_1\right)}$$`

* We would reject `\(H_0\)` at the `\(\alpha\)` level of significance whenever `\(|z_{obs}| &gt; z_{1 - \alpha/2}\)`, where `\(z_{1 - \alpha/2}\)` is the quantile from a standard normal distribution

    - A typical value is 1.96 which corresponds to `\(\alpha = 0.05\)`
    
    ```r
    qnorm(0.975)
    ```
    
    ```
    ## [1] 1.959964
    ```


---

## Multiple logistic regression

* We saw how to extend the SLR model to include more than one predictor (e.g., polynomial regression)

* The same can be done for logistic regression in an analogous manner

* Suppose we have `\(p\)` predictors: `\(x_1, x_2, \dots, x_p\)`

* In multiple logistic regression, the probability of success is modeled using `$$p\left(x_1, x_2, \dots, x_p\right) = \frac{exp\left(\beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_px_p\right)}{1 + exp\left(\beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_px_p\right)}$$`

* Or, equivalently, `$$logit\left[p\left(x_1, x_2, \dots, x_p\right)\right] = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_px_p$$`


---

## Significance testing

* In ordinary linear regression, the SSE/MSE/RMSE measure how "close" the predicted values from the fitted model are to the the observed outcomes

* In logistic regression (and GzLMs in general), we use a statistic called the *deviance*, denoted `\(D\)`

* In general, the deviance is defined as `\(D = 2 \left(l_{saturated} - l_{proposed}\right)\)`, where

    - `\(l_{saturated}\)` is the log-likelihood for the *saturated* model (i.e., a model with as many parameters as observations)
    
    - `\(l_{proposed}\)` is the log-likelihood for the proposed model under consideration
    
* If the proposed model is "good", then `\(D\)` should be small and will **approximately** follow a chi-squared distribution on `\(n - p - 1\)` degrees of freedom (think od `\(D\)` as a measure of "badness of fit"; that is, larger is worse)

    - This is an asymptotic result, meaning it is only reliable with "large enough"" sample sizes!
    
* Much like the `\(F\)`-tests in ordinary linear regression, we can use the deviance and chi-squared tests to compare nested models


---

## Plant inbreeding example



```r
# Load the data
inbreeding &lt;- read.table(
  file = "/Users/bgreenwell/Dropbox/teaching/stt7140-env/data/inbreeding.dat",
  col.names = c("damage", "inbred", "y", "biomass")
)
head(inbreeding)  # print first few observations
```

```
##   damage inbred y biomass
## 1 0.2138      0 1  0.0525
## 2 0.2138      0 1  0.0920
## 3 0.2138      1 1  0.0239
## 4 0.2138      0 0  0.0149
## 5 0.0000      0 0  0.0410
## 6 0.3907      0 0  0.0264
```


---

## Plant inbreeding example

.scrollable[


```r
# Fit a logistic regression model
fit &lt;- glm(
  formula = y ~ as.factor(inbred) + biomass + damage, 
  data = inbreeding,
  family = binomial(link = "logit")
)
summary(fit)  # print model summary
```

```
## 
## Call:
## glm(formula = y ~ as.factor(inbred) + biomass + damage, family = binomial(link = "logit"), 
##     data = inbreeding)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.06793  -0.89987   0.00085   1.10251   1.77324  
## 
## Coefficients:
##                    Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)         -1.1230     0.6795  -1.653   0.0984 . 
## as.factor(inbred)1   0.3110     0.6821   0.456   0.6484   
## biomass             41.3349    15.9374   2.594   0.0095 **
## damage              -1.8555     2.1420  -0.866   0.3864   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 69.235  on 49  degrees of freedom
## Residual deviance: 52.463  on 46  degrees of freedom
## AIC: 60.463
## 
## Number of Fisher Scoring iterations: 6
```

]


---

## Plant inbreeding example

.scrollable[


```r
# Fit a logistic regression model
fit2 &lt;- update(fit, y ~ biomass)
anova(fit2, fit, test = "Chisq")  # chi-squared test comparing models
```

```
## Analysis of Deviance Table
## 
## Model 1: y ~ biomass
## Model 2: y ~ as.factor(inbred) + biomass + damage
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
## 1        48     53.579                     
## 2        46     52.463  2   1.1164   0.5722
```

]

* What statistical hypothesis is being tested here?


---

## Generlized linear models (GLMs)

* Recall that the MLR model is fitting a conditional mean of the form `$$\mu = E\left(y|\boldsymbol{x}\right) = \beta_0 + \sum_{j = 1}^p \beta_j x_j$$`

    - We refer to `\(\beta_0 + \sum_{j = 1}^p \beta_j x_j\)` as the *linear predictor*

* This includes, as special cases, factorial ANOVAs, one- and two-sample `\(t\)`-tests, ANOCOVA, polynomial models, etc.

* The mean of the (continuous) response is modeled as a **linear** function of the `\(\beta_j\)`s `\(\left(j = 1, 2, \dots, p\right)\)`

* The response `\(y\)` is assumed to have a normal distribution with mean `\(\mu = E\left(y|\boldsymbol{x}\right)\)` and constant variance `\(\sigma^2\)`

* In GLMs, the mean response is "linked" to the linear predictor via a *link function*


---

## Generlized linear models (GLMs)

* GLMs have two important characteristics:

  1) The response consists of a random sample from an *exponential distribution*
    
  2) The mean response `\(\mu\)` is related to the linear predictor through a link function `\(g\left(\cdot\right)\)` `$$g\left(\mu\right) = g\left(E\left[y|\boldsymbol{x}\right]\right) = \beta_0 + \sum_{j = 1}^p \beta_j x_j$$` 
  
* For the MLR model, the link function is just the identity function `\(g\left(x\right) = x\)`

* For logistic regression, `\(\mu = E\left[y|\boldsymbol{x}\right] = p\left(x\right)\)`, and the link function is the logit


---

## Poisson regression

* Another special case of the GLM that is useful in practice is the Poisson GLM

* The Poisson distribution is a discrete distribution that is useful for modelling count data

* For example, Venables and Ripley (2002) provide some data on the number of new AIDS cases each year, in Belgium, from 1981 onward

* If we let `\(Y\)` represent the number of new AIDS cases in a particular year, then `\(Y \sim Poi\left(\mu = 1 / \lambda\right)\)`; that is, `\(Y\)` is said to have a Poisson distribution with mean `\(\mu\)` or rate parameter `\(\lambda\)`

* Poisson models involve three assumptions:

  1) The occurrences of the event of interest in non-overlapping "time" intervals are independent
  
  2) The probability that two or more events will happen in a small interval of time is small
  
  3) The probability that an event occurs in a short interval of times is proportional to the length of the time interval


---

## Poisson regression

* In a Poisson GLM, the mean `\(\mu\)` response is related to the linear predictor through a (natural) `\(\log\)` link `$$\log\left(\mu\right) = \beta_0 + \sum_{j = 1}^p \beta_j x_j$$`

    - The `\(\log\)` link is a natural choice for count data since counts are always positive
    
    - This is also an example of a *log-linear* model which are useful for, among other things, analyzing complex contingency tables
    
* **Note:** the Poisson model does NOT assume constant variance!

* The regression coefficients can be estimated via maximum likelihood estimation

* The likelihood function being maximized is `\(L\left(\boldsymbol{\beta}\right) = \prod_{i = 1}^n \frac{\exp\left(-\mu_i\right)\mu_i^{y_i}}{y_i!}\)`, where `\(\mu_i = \exp\left(\beta_0 + \sum_{j = 1}^p \beta_j x_j\right)\)`
    </textarea>
<script src="libs/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();
(function(time) {
  var d2 = function(number) {
    return ('0' + number).slice(-2); // left-pad 0 to minutes/seconds
  },

  time_format = function(total) {
    var secs = Math.abs(total) / 1000;
    var h = Math.floor(secs / 3600);
    var m = Math.floor(secs % 3600 / 60);
    var s = Math.round(secs % 60);
    var res = d2(m) + ':' + d2(s);
    if (h > 0) res = h + ':' + res;
    return res;  // [hh:]mm:ss
  },

  slide_number_div = function(i) {
    return document.getElementsByClassName('remark-slide-number').item(i);
  },

  current_page_number = function(i) {
    return slide_number_div(i).firstChild.textContent;  // text "i / N"
  };

  var timer = document.createElement('span'); timer.id = 'slide-time-left';
  var time_left = time, k = slideshow.getCurrentSlideIndex(),
      last_page_number = current_page_number(k);

  setInterval(function() {
    time_left = time_left - 1000;
    timer.innerHTML = ' ' + time_format(time_left);
    if (time_left < 0) timer.style.color = 'red';
  }, 1000);

  slide_number_div(k).appendChild(timer);

  slideshow.on('showSlide', function(slide) {
    var i = slide.getSlideIndex(), n = current_page_number(i);
    // reset timer when a new slide is shown and the page number is changed
    if (last_page_number !== n) {
      time_left = time; last_page_number = n;
      timer.innerHTML = ' ' + time_format(time); timer.style.color = null;
    }
    slide_number_div(i).appendChild(timer);
  });
})(60000);</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
