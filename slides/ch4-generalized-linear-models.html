<!DOCTYPE html>
<html>
  <head>
    <title>Generalized Linear Models (GzLMs)</title>
    <meta charset="utf-8">
    <meta name="author" content="Brandon M. Greenwell" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="scrollable.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Generalized Linear Models (GzLMs)
## <br/>ES/STT 7140: Statistical Modeling for Environmental Data
### Brandon M. Greenwell

---




## Introduction

* In ordinary linear regression, we assume that the response `\(y\)` is continuous (and normally distributed)

* In many real word regression problems, however, the response may not be normally distributed, or even continuous!

* For example, many response variables of interest are binary (e.g., `"survived"` or `"died"`, `"infected"` or `"not infected"`)

* Another common type of response is a count (e.g., the number of eggs in a nest or number of vines on a tree)

    - Although not normal, we can use the *Poisson* and *negative binomial* distributions to model counts
    
* In this chapter, we introduce GzLMs for accommodating many types of non-normal response variables

* We start with the most common type of GzLM for handling binary outcomes


---
class: inverse, center, middle


# Logistic Regression

---

## Binary outcomes

* Consider an experiment where the measured outcome of interest is either a success or failure (which can be coded as 1 and 0, respectively)

* The probability of a success or failure may depend on a set of predictor variables

* One idea on how to model such data is to simply fit a regression with the goal of
estimating the probability of success given some values of the predictor 

* However, this approach will not work because probabilities are constrained to fall between 0 and 1

* In the classical regression setup with a continuous response, the predicted values can range over all real numbers

* For binary outcomes, a different modeling technique is needed


---

## Binary outcomes

* Recall that fitting an SLR model is equivalent to estimating a conditional mean of the form `$$E\left(y|x\right) = \beta_0 + \beta_1 x$$`

* In logistic regression, we have `\(E\left(y|x\right) = P\left(y = 1|x\right)\)`

    - In other words, in logistic regression, we are estimating a conditional probability
    
* The goal in logistic regression is to model the probability that `\(y = 1\)`

    - If we know `\(P\left(y = 1|x\right)\)` then we also know `\(P\left(y = 0|x\right) = 1 - P\left(y = 1|x\right)\)`


---

## Dobson's beetle data

Beetles were exposed to gaseous carbon disulphide at various concentrations
(in mf/L) for five hours (Bliss, 1935) and the number of beetles killed were
noted. The data are available in the [`investr`](https://cran.r-project.org/package=investr) package.


```r
*# install.packages("investr")
data(beetle, package = "investr")
beetle
```

```
##    ldose  n  y
## 1 1.6907 59  6
## 2 1.7242 60 13
## 3 1.7552 62 18
## 4 1.7842 56 28
## 5 1.8113 63 52
## 6 1.8369 59 53
## 7 1.8610 62 61
## 8 1.8839 60 60
```


---

## Dobson's beetle data


```r
plot(y/n ~ ldose, data = beetle, pch = 19, cex = 1.2)
```

&lt;img src="figures/unnamed-chunk-2-1.svg" style="display: block; margin: auto;" /&gt;


---

## Binomial experiments

* **Binomial experiments** are experiments which consist of:

    (1) `\(n\)` *independent* trials
    
    (2) Each trial results in a binary outcome (typically defined as success or failure)
    
    (3) The probability of sucess, denoted `\(p\)`, is the same for each trial
    
* Can you think of a simple binomial experiment?

* The **binomial distribution** describes the probability of observing `\(k\)` successes in a binomial experiment with `\(n\)` (independent) trials and probability of success `\(p\)`

    - The binomial distribution is denoted `\(Bin\left(n, p\right)\)`


---

## Logistic regression

* Logistic regression goes a step further by allowing the probability of success to be a function of `\(x\)` (i.e., a binomial distribution where `\(p\)` depends on `\(x\)`)

* In (simple) logistic regression, we model the probability of success as a function of `\(x\)` using `$$p\left(x\right) = \frac{\exp\left(\beta_0 + \beta_1 x\right)}{1 + \exp\left(\beta_0 + \beta_1 x\right)}$$`

    - If `\(\beta_1 = 0\)` then `\(p\left(x\right)\)` is a constant
    
    - If `\(\beta_1 &lt; 0\)`, then `\(p\left(x\right)\)` is an increasing function in `\(x\)` (and vice versa if `\(\beta_1 &lt; 0\)`)
    
    - For a particular probability of success `\(p\)`, the *odds of success* is defined as `\(p / \left(1 - p\right)\)`

--

* In logistic regression, we are modelling the `\(\log\left(odds\right)\)`, also referred to as the *logit*: `$$\log\left[\frac{p\left(x\right)}{1 - p\left(x\right)}\right] = \beta_0 + \beta_1 x = \log\left[odds\left(x\right)\right]$$`


---

## Maximum likelihood estimation

* The question now is "How do we estimate `\(\beta_0\)` and `\(\beta_1\)`?"

* The method of *maximum likelihood* (ML) estimation is a general technique for estimating the parameters of a distribution from observed data

* For linear regression with normally distributed errors the ML estimates of the parameters are equivalent to the LS estimates

    - ML estimation also provides an estimate of `\(\sigma\)` in linear regression!

* A general overview of ML estimation is beyond the scope of this class, but we'll illustrate with a simple example: 

    - Finding the ML estimator of `\(p\)` in a binomial experiment

* Typically, however, closed-form solutions are not available for most problems and iterative numerical techniques are required in finding ML estimates (e.g., the *Newton-Raphson algorithm* and *iterative re-weighted least squares*)

---

## Logistic regression in R

.scrollable[

* In R, we can fit the logistic regression models using the `glm()` function

* `glm()` is a general function for fitting all kinds of GzLMs (including ordinary linear regression models!)

* To specify a GzLM you need to specify some additional arguments (the most important being the `family` argument)

* For logistic regression we always specify `family = binomial` in the call to `glm()`

* The template for simple logistic regression is

```r
fit &lt;- glm(y ~ x, data = &lt;data-name&gt;, family = binomial)
```
where `y` is a factor with the first level denoting a "success" or 

```r
fit &lt;- glm(cbind(y, n-y) ~ x, data = &lt;data-name&gt;, family = binomial)
```
where `n` and `y` are the number of trials and successes, respectively, in a binomial experiment

]


---

## Dobson's beetle data

.scrollable[


```r
fit &lt;- glm(
* cbind(y, n-y) ~ ldose,
  data = beetle,
* family = binomial(link = "logit")
)
summary(fit)
```

```
## 
## Call:
## glm(formula = cbind(y, n - y) ~ ldose, family = binomial(link = "logit"), 
##     data = beetle)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.5941  -0.3944   0.8329   1.2592   1.5940  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -60.717      5.181  -11.72   &lt;2e-16 ***
## ldose         34.270      2.912   11.77   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 284.202  on 7  degrees of freedom
## Residual deviance:  11.232  on 6  degrees of freedom
## AIC: 41.43
## 
## Number of Fisher Scoring iterations: 4
```

]


---

## Interpeting the coefficients

* So how do we interpret `\(\beta_0\)` and `\(\beta_1\)` in the (simple) logistic regression model?

* Recall that in the SLR model, `\(\beta_1\)` represents the rate of change in the mean response per one-unit increase in `\(x\)`

* In the (simple) logistic regression model, however, we don't interpret `\(\beta_1\)` directly, but rather, `\(\exp\left(\beta_1\right)\)`

* As it turns out, for a continuous predictor `\(x\)`, `$$OR = \frac{odds\left(x + 1\right)}{odds\left(x\right)} = \exp\left(\beta_1\right)$$`

* Hence, for a continuous predictor `\(x\)`, `\(\exp\left(\beta_1\right)\)` represents the multiplicative 


---

## Inference for the slope

* As was the case with SLR, we are often concerned with the statistical significance of the slope; in particular, whether or not it statistically significantly differs from zero: `$$H_0: \beta_1 = 0 \quad versus \quad H_1: \beta_1 \ne 0$$`

* The simplest approach is to use a *Wald test*

* The Wald test for `\(\beta_1\)` in the logistic regression model is based on the test statistic `$$z = \frac{\widehat{\beta}_1}{\widehat{SE}\left(\widehat{\beta}_1\right)}$$`

* We would reject `\(H_0\)` at the `\(\alpha\)` level of significance whenever `\(|z| &gt; z_{1 - \alpha/2}\)`, where `\(z_{1 - \alpha/2}\)` is the quantile from a standard normal distribution

    - A typical value is 1.96 which corresponds to `\(\alpha = 0.05\)`
    
    ```r
    qnorm(0.975)
    ```
    
    ```
    ## [1] 1.959964
    ```
    </textarea>
<script src="libs/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();
(function(time) {
  var d2 = function(number) {
    return ('0' + number).slice(-2); // left-pad 0 to minutes/seconds
  },

  time_format = function(total) {
    var secs = Math.abs(total) / 1000;
    var h = Math.floor(secs / 3600);
    var m = Math.floor(secs % 3600 / 60);
    var s = Math.round(secs % 60);
    var res = d2(m) + ':' + d2(s);
    if (h > 0) res = h + ':' + res;
    return res;  // [hh:]mm:ss
  },

  slide_number_div = function(i) {
    return document.getElementsByClassName('remark-slide-number').item(i);
  },

  current_page_number = function(i) {
    return slide_number_div(i).firstChild.textContent;  // text "i / N"
  };

  var timer = document.createElement('span'); timer.id = 'slide-time-left';
  var time_left = time, k = slideshow.getCurrentSlideIndex(),
      last_page_number = current_page_number(k);

  setInterval(function() {
    time_left = time_left - 1000;
    timer.innerHTML = ' ' + time_format(time_left);
    if (time_left < 0) timer.style.color = 'red';
  }, 1000);

  slide_number_div(k).appendChild(timer);

  slideshow.on('showSlide', function(slide) {
    var i = slide.getSlideIndex(), n = current_page_number(i);
    // reset timer when a new slide is shown and the page number is changed
    if (last_page_number !== n) {
      time_left = time; last_page_number = n;
      timer.innerHTML = ' ' + time_format(time); timer.style.color = null;
    }
    slide_number_div(i).appendChild(timer);
  });
})(60000);</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
